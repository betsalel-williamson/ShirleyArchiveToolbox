# Diary Archival Project: The Shirley Pearl Collection

## 1. Project Overview

This project aims to transform the rich, unstructured historical data from a physical diary into a structured, queryable, and analyzable digital format. The initial focus is on the multi-year diary of Shirley Pearl, written during the 1940s.

The primary goal is to create a high-fidelity dataset that serves as a clean, reliable source for a modern data stack, enabling historical analysis, data visualization, and long-term digital preservation.

## 2. The Source Material

The source data consists of scanned images starting with the first diary from a "Five-Year Diary" kept by Shirley Pearl. This diary is a rich tapestry of a young woman's daily life during the World War II era, containing valuable insights into social dynamics, education, family life, and popular culture.

### Diary 1, Five-Year Diary 1943-1944

**Key Characteristics:**

* **Multi-Year Format:** A single page contains entries for the same calendar date across multiple years (1943-1948).
* **Rich Cultural Context:** The text is filled with 1940s slang ("wormy," "sharpies"), references to public figures (Frank Sinatra, Roosevelt), media (movies, radio shows), and social clubs ("The Arcadettes").
* **Mixed Writing Systems:** The diary is primarily written in English cursive but also contains instances of Gregg Shorthand.

## 3. Methodology: LLM-Powered Data Extraction

We have established a robust pipeline for converting diary images into structured data using a Large Language Model (LLM).

The workflow is as follows:

`Scanned Image` -> `LLM with a Structured Prompt` -> `Validated JSON Output`

The cornerstone of this process is a highly detailed **System Prompt** that instructs the LLM to act as an expert archivist. This prompt includes:

1. A strict **OpenAPI 3.0 schema** that the output JSON must conform to.
2. A set of **specific rules** tailored to the nuances of Shirley Pearl's diary.

## 4. The Data Schema: Our Core Data Contract

The output of the extraction process is a JSON file for each processed page, defined by a master OpenAPI schema (`schema.yaml`). This schema is the canonical definition of our data structure.

The JSON is organized into three main sections:

* `"metadata"`: Contains high-level information about the diary as a whole, including the author, date range, physical description, and a list of key identified groups or clubs (`key_entities`).
* `"people_index"`: A comprehensive, normalized list of every person mentioned in the diary. This acts as a central dimension table, assigning a unique `person_id` and consolidating aliases (e.g., "Flo" and "Florence") and relationships (e.g., "Friend," "Teacher").
* `"entries"`: An array of every individual diary entry. Each entry is a rich object containing the verbatim text, a summary, sentiment analysis, and structured links to the people and locations mentioned.

### Key Schema Design Features

* **Normalization:** The `people_index` prevents data duplication and allows for powerful relational analysis (e.g., "show me all entries mentioning teachers").
* **Contextual Annotations:** The `annotations` field within each entry is designed to capture ephemeral but vital cultural information, such as the meaning of slang terms or explanations of shorthand symbols, without polluting the primary text.
* **Multi-Language/Script Support:** The schema explicitly accounts for multiple languages and writing systems at both the metadata and entry level, ensuring no information is lost.

## 5. Tooling and Workflow

* **Prompt Templating:** We use the `m4` macro processor to build the final system prompt. This decouples the complex OpenAPI schema definition from the instructional text, allowing for easier maintenance.
* **Workflow:**
    1. The master schema is maintained in `schema.yaml`.
    2. The prompt text and rules are maintained in `prompt_template.md.m4`.
    3. The final prompt is generated by running:

        ```bash
        m4 prompt_template.md.m4 > system_prompt.md
        ```

## 6. Project Status & Next Steps (Handoff to Dagster/dbt)

**Current Status:** The initial data extraction methodology, schema design, and prompting strategy have been successfully developed and tested. We have a repeatable process for generating high-quality JSON data from diary images.

This project is now ready for the next phase: ingestion and modeling.

### Ingestion with Dagster

The generated JSON files are the perfect source assets for a Dagster pipeline. The goal will be to orchestrate the loading of this data into a data warehouse (e.g., Snowflake, BigQuery, DuckDB).

* **Assets:** Define a Dagster asset for the raw JSON files (e.g., in an S3 bucket or local storage).
* **Orchestration:** Create downstream assets that parse these JSON files and load their contents into raw staging tables in the warehouse. Dagster will ensure this process is reliable, observable, and repeatable.

### Modeling with dbt

Once the data is loaded into staging tables, dbt will be used to transform it into a clean, analytics-ready format. A potential dbt model structure could be:

1. **Staging Models:**
    * `stg_diary_entries`: Cleans and casts data from the raw `entries` array.
    * `stg_people`: Cleans data from the `people_index`.
    * `stg_annotations`: Unnests the `annotations` array into a flat table.

2. **Dimension and Fact Models (Star Schema):**
    * `dim_people`: A dimension table containing the final state of each person from the `people_index`.
    * `dim_dates`: A standard date dimension table.
    * `fct_diary_entries`: The central fact table, with one row per diary entry. It would contain foreign keys to `dim_people` (for the author), `dim_dates`, and metrics like word count, along with the entry's text and summary.

This layered approach will provide a powerful and intuitive dataset for any future analyst or historian.

## 7. Key Files in This Repository

* `schema.yaml`: The master OpenAPI 3.0 schema for the JSON output.
* `prompt_template.md.m4`: The `m4` template file for the LLM system prompt.
* `system_prompt.md`: The final, generated system prompt (for reference).
* `data/`: Directory containing the extracted JSON files from processed diary pages.
* `images/`: Directory containing the source scanned images of the diary.


## Tasks and Bugs

- **BUG** the gemini building of boxes does so on a transformed image, that isn't helpful because the final data is skewed from the original image, need to conside a better way to get the bounding boxes from the image through the gen ai.